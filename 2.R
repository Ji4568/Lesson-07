###第二節：線性迴歸(1)
#線性迴歸(linear regression)是高中已經教過的基本分析技術，目標是從一群數據點中找出xi與yi之間的線性關係，其方程式如下：
#這裡在高中上課的時候有提到我們要如何解這個b0

#線性迴歸(2)
#想要求得某一式子的極值是件很容易的事情，我們只要使用微分工具即可，如此我們就能找出能使?????2最小化的b0與b1的係數：

#對b0偏微分的求解過程如下：

#對b1偏微分的求解過程如下：

#最終我們將能獲得這樣的兩個聯立方程式：

#線性迴歸(3)
#解開上述那兩個聯立方程式後，我們會獲得這樣的結果：
#這個式子看起來很難懂，但仔細一看你會發現它可以被簡化成這樣：

#線性迴歸(4)
#讓我們根據這個式子進行計算吧：
x = c(1, 2, 3, 4, 5)
y = c(6, 7, 9, 8, 10)

b1 = cov(x, y)/var(x)
b1

b0 = mean(y) - b1 * mean(x)
b0

#得到答案以後，讓我們試試R裡面的函數「lm」，你會發現你能得到完全一樣的結果：
model = lm(y~x)
model

#所以以後其實我們要做這件事情其實用函數「lm」就可以了，我們試著把裡面的係數叫出來。
model$coefficients

#線性迴歸(5)
#假設我們使用的是剛剛的資料，讓我們看看使用函數「lm」的方法：
lm(SBP~DBP, data = dat)

lm(dat[,"eGFR"]~dat[,"SBP"])

#比較有趣的地方是，我們其實可以同時使用多個變數一起預測SBP：
lm(dat[,"eGFR"] ~ dat[,"SBP"] + dat[,"DBP"])

#為了應付可能需要擴充變項，你需要學會用這種方法來進行預測（這邊要特別注意drop=FALSE的效果）：
lm(dat[,"eGFR"] ~ ., data = dat[,c("SBP", "DBP"),drop=FALSE])





